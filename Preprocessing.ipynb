{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl3b_abVQpZ6"
      },
      "outputs": [],
      "source": [
        "!pip install pandas jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7imxdkqyW_nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/cs774/WDC\"\n",
        "for zip_name in [\"20pair.zip\", \"50pair.zip\", \"80pair.zip\"]:\n",
        "    zip_path = os.path.join(base_path, zip_name)\n",
        "    extract_dir = os.path.join(base_path, 'jaccard', zip_name.replace(\".zip\", \"\"))\n",
        "    print(f\"Unzipping {zip_name} to {extract_dir}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"Done extracting to: {extract_dir}\")"
      ],
      "metadata": {
        "id": "KcStVehgYjMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preview of Pre-processed data"
      ],
      "metadata": {
        "id": "X3fU2PjLoILi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_data_fields(file_path, num_samples=10):\n",
        "    \"\"\"Explore all available fields in the data by looking at sample records.\"\"\"\n",
        "    print(f\"\\nExploring fields in: {file_path}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    all_fields = set()\n",
        "    samples = []\n",
        "\n",
        "    # Read a few records and collect all field names\n",
        "    with gzip.open(file_path, 'rt', encoding='utf-8') as read:\n",
        "        count = 0\n",
        "        for line in read:\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "\n",
        "            entry = json.loads(line)\n",
        "            samples.append(entry)\n",
        "            all_fields.update(entry.keys())\n",
        "            count += 1\n",
        "\n",
        "    # Print all discovered fields\n",
        "    print(f\"Discovered {len(all_fields)} fields:\")\n",
        "    for field in sorted(all_fields):\n",
        "        print(f\"- {field}\")\n",
        "\n",
        "    # Print a complete sample record\n",
        "    if samples:\n",
        "        print(\"\\nSample record (complete):\")\n",
        "        print(json.dumps(samples[0], indent=2))\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    return all_fields\n",
        "\n",
        "# Example usage - add this before your main loop\n",
        "sample_folder = os.path.join(base_dir, list(splits.keys())[0])\n",
        "sample_files = [f for f in os.listdir(sample_folder) if f.endswith(\".json.gz\") and \"train\" in f]\n",
        "if sample_files:\n",
        "    all_fields = explore_data_fields(os.path.join(sample_folder, sample_files[0]))"
      ],
      "metadata": {
        "id": "oMRBONgomFPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "import jsonlines\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/cs774/WDC/jaccard\"\n",
        "\n",
        "splits = {\n",
        "    \"20pair\": \"20\",\n",
        "    \"50pair\": \"50\",\n",
        "    \"80pair\": \"80\"\n",
        "}\n",
        "\n",
        "# Load exchange rates from CSV into a dictionary\n",
        "def load_exchange_rates():\n",
        "    exchange_rates_path = os.path.join(\"/content/drive/MyDrive/cs774/WDC\", \"exchange_rates\", \"xrate_april_2025.csv\")\n",
        "    rates_df = pd.read_csv(exchange_rates_path)\n",
        "    rates_dict = {}\n",
        "\n",
        "    if not rates_df.empty:\n",
        "        rate_row = rates_df.iloc[0]\n",
        "        for currency in rates_df.columns:\n",
        "            if currency != 'Date' and currency.strip():\n",
        "                try:\n",
        "                    usd_rate = float(rate_row.get(' USD', 1.0))\n",
        "                    currency_rate = float(rate_row.get(currency))\n",
        "                    rates_dict[currency.strip()] = currency_rate / usd_rate\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "        rates_dict['USD'] = 1.0\n",
        "\n",
        "    return rates_dict\n",
        "\n",
        "EXCHANGE_RATES = load_exchange_rates()\n",
        "PRICE_RANGE_TOLERANCE = 0.10  # 10% tolerance\n",
        "\n",
        "# Format product data with price normalization and range matching\n",
        "def serialize_product(entry, side):\n",
        "    fields = ['title', 'brand', 'description']\n",
        "    parts = []\n",
        "\n",
        "    # Handle price normalization and range matching\n",
        "    price = entry.get(f\"price_{side}\")\n",
        "    currency = entry.get(f\"priceCurrency_{side}\")\n",
        "\n",
        "    if price and price != \"null\" and currency and currency != \"null\":\n",
        "        try:\n",
        "            parts.append(f\"price: {price}\")\n",
        "            parts.append(f\"priceCurrency: {currency}\")\n",
        "\n",
        "            price_value = float(price)\n",
        "            conversion_rate = EXCHANGE_RATES.get(currency, 1.0)\n",
        "            normalized_price = price_value * conversion_rate\n",
        "\n",
        "            lower_bound = normalized_price * (1 - PRICE_RANGE_TOLERANCE)\n",
        "            upper_bound = normalized_price * (1 + PRICE_RANGE_TOLERANCE)\n",
        "\n",
        "            parts.append(f\"price_usd: {normalized_price:.2f}\")\n",
        "            parts.append(f\"price_range: {lower_bound:.2f}-{upper_bound:.2f} USD\")\n",
        "        except (ValueError, TypeError):\n",
        "            parts.append(f\"price: {price}\")\n",
        "            parts.append(f\"priceCurrency: {currency}\")\n",
        "\n",
        "    # Add other product fields\n",
        "    for field in fields:\n",
        "        key = f\"{field}_{side}\"\n",
        "        val = entry.get(key)\n",
        "        if val and val != \"null\":\n",
        "            parts.append(f\"{field}: {val}\")\n",
        "\n",
        "    return \" || \".join(parts)\n",
        "\n",
        "# Main processing loop: convert files across all data splits\n",
        "for folder_name, tag in splits.items():\n",
        "    folder_path = os.path.join(base_dir, folder_name)\n",
        "    for fname in os.listdir(folder_path):\n",
        "        if fname.endswith(\".json.gz\") and (\"train\" in fname or \"valid\" in fname):\n",
        "            input_path = os.path.join(folder_path, fname)\n",
        "            output_name = fname.replace(\".json.gz\", f\"_ditto_{tag}.jsonl\")\n",
        "            output_path = os.path.join(folder_path, output_name)\n",
        "\n",
        "            print(f\"Converting {fname} â†’ {output_name}...\")\n",
        "            with gzip.open(input_path, 'rt', encoding='utf-8') as read, jsonlines.open(output_path, mode='w') as writer:\n",
        "                for line in read:\n",
        "                    raw = json.loads(line)\n",
        "                    writer.write({\n",
        "                        \"text_left\": serialize_product(raw, \"left\"),\n",
        "                        \"text_right\": serialize_product(raw, \"right\"),\n",
        "                        \"label\": str(raw[\"label\"])\n",
        "                    })\n",
        "            print(f\"Saved: {output_path}\")"
      ],
      "metadata": {
        "id": "1ZuV1BnHUQXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Convert .jsonl into dataframe\n",
        "\n",
        "import pandas as pd\n",
        "import jsonlines\n",
        "\n",
        "def jsonl_to_dataframe(file_path):\n",
        "  \"\"\"Converts a JSONL file to a Pandas DataFrame.\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the JSONL file.\n",
        "\n",
        "  Returns:\n",
        "    A Pandas DataFrame representing the data in the JSONL file.\n",
        "    Returns None if there's an error during file processing.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    data = []\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "      for obj in reader:\n",
        "        data.append(obj)\n",
        "    return pd.DataFrame(data)\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    return None\n",
        "\n",
        "# Example usage (assuming 'output_path' from your previous code):\n",
        "# Replace with your actual file path\n",
        "# Example file path (modify to your specific jsonl file)\n",
        "file_path = \"/content/drive/MyDrive/cs774/WDC/80pair/wdcproducts80cc20rnd000un_train_large_ditto_80.jsonl\"\n",
        "\n",
        "df = jsonl_to_dataframe(file_path)\n",
        "\n",
        "if df is not None:\n",
        "    print(df.head())\n",
        "    print(df.info())\n"
      ],
      "metadata": {
        "id": "QJcO6qlxKrkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df.head(5).to_csv(\"sample.csv\")"
      ],
      "metadata": {
        "id": "2i8txJS7K1z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preview of the processed data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "axWfr04dSLdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preview_converted_data(file_path, num_samples=5):\n",
        "    print(f\"\\nPreviewing {num_samples} entries from converted data: {file_path}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    count = 0\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for entry in reader:\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "\n",
        "            print(f\"Entry #{count+1}:\")\n",
        "            print(f\"Left Product: {entry.get('text_left', 'N/A')}\")\n",
        "            print(f\"Right Product: {entry.get('text_right', 'N/A')}\")\n",
        "            print(f\"Match: {entry.get('label', 'N/A')}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            count += 1\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "WYV4RFIyalVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_folder = os.path.join(base_dir, list(splits.keys())[0])  # Use first split folder\n",
        "sample_files = [f for f in os.listdir(sample_folder) if f.endswith(\"_ditto_\" + splits[list(splits.keys())[0]] + \".jsonl\") and \"train\" in f]\n",
        "if sample_files:\n",
        "    preview_converted_data(os.path.join(sample_folder, sample_files[0]))"
      ],
      "metadata": {
        "id": "7k-4V1gIRuIJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}