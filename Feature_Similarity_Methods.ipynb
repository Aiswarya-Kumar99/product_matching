{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Jaccard Similarity, String Similarity, Weighted Ensemble Similarity"
      ],
      "metadata": {
        "id": "qe4Ii-ESRDna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl3b_abVQpZ6"
      },
      "outputs": [],
      "source": [
        "!pip install pandas jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7imxdkqyW_nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/cs774/WDC\"\n",
        "for zip_name in [\"20pair.zip\", \"50pair.zip\", \"80pair.zip\"]:\n",
        "    zip_path = os.path.join(base_path, zip_name)\n",
        "    extract_dir = os.path.join(base_path, zip_name.replace(\".zip\", \"\"))\n",
        "    print(f\"Unzipping {zip_name} to {extract_dir}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"Done extracting to: {extract_dir}\")"
      ],
      "metadata": {
        "id": "KcStVehgYjMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preview of Pre-processed data"
      ],
      "metadata": {
        "id": "X3fU2PjLoILi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_data_fields(file_path, num_samples=10):\n",
        "    \"\"\"Explore all available fields in the data by looking at sample records.\"\"\"\n",
        "    print(f\"\\nExploring fields in: {file_path}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    all_fields = set()\n",
        "    samples = []\n",
        "\n",
        "    # Read a few records and collect all field names\n",
        "    with gzip.open(file_path, 'rt', encoding='utf-8') as read:\n",
        "        count = 0\n",
        "        for line in read:\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "\n",
        "            entry = json.loads(line)\n",
        "            samples.append(entry)\n",
        "            all_fields.update(entry.keys())\n",
        "            count += 1\n",
        "\n",
        "    # Print all discovered fields\n",
        "    print(f\"Discovered {len(all_fields)} fields:\")\n",
        "    for field in sorted(all_fields):\n",
        "        print(f\"- {field}\")\n",
        "\n",
        "    # Print a complete sample record\n",
        "    if samples:\n",
        "        print(\"\\nSample record (complete):\")\n",
        "        print(json.dumps(samples[0], indent=2))\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    return all_fields\n",
        "\n",
        "sample_folder = os.path.join(base_dir, list(splits.keys())[0])\n",
        "sample_files = [f for f in os.listdir(sample_folder) if f.endswith(\".json.gz\") and \"train\" in f]\n",
        "if sample_files:\n",
        "    all_fields = explore_data_fields(os.path.join(sample_folder, sample_files[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMRBONgomFPc",
        "outputId": "63892864-7418-4157-e0df-a1bff360f751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exploring fields in: /content/drive/MyDrive/cs774/WDC/20pair/wdcproducts20cc80rnd000un_train_small.json.gz\n",
            "--------------------------------------------------------------------------------\n",
            "Discovered 17 fields:\n",
            "- brand_left\n",
            "- brand_right\n",
            "- cluster_id_left\n",
            "- cluster_id_right\n",
            "- description_left\n",
            "- description_right\n",
            "- id_left\n",
            "- id_right\n",
            "- is_hard_negative\n",
            "- label\n",
            "- pair_id\n",
            "- priceCurrency_left\n",
            "- priceCurrency_right\n",
            "- price_left\n",
            "- price_right\n",
            "- title_left\n",
            "- title_right\n",
            "\n",
            "Sample record (complete):\n",
            "{\n",
            "  \"id_left\": 70706895,\n",
            "  \"brand_left\": null,\n",
            "  \"title_left\": \"Western Digital Blue SSD 3D 2TB 2.5 (WDS200T2B0A)\",\n",
            "  \"description_left\": null,\n",
            "  \"price_left\": null,\n",
            "  \"priceCurrency_left\": null,\n",
            "  \"cluster_id_left\": 1455368,\n",
            "  \"id_right\": 52586114,\n",
            "  \"brand_right\": null,\n",
            "  \"title_right\": \"Western Digital Blue PC 2.5\\\\\\\" 250 GB Serial ATA III\",\n",
            "  \"description_right\": \"250 GB, 2.5\\\\\\\", SATA 6Gb/s, 540/500 MB/s\",\n",
            "  \"price_right\": \"62.436001\",\n",
            "  \"priceCurrency_right\": \"EUR\",\n",
            "  \"cluster_id_right\": 483248,\n",
            "  \"pair_id\": \"70706895#52586114\",\n",
            "  \"label\": 0,\n",
            "  \"is_hard_negative\": true\n",
            "}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "import jsonlines\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/cs774/WDC\"\n",
        "\n",
        "splits = {\n",
        "    \"20pair\": \"20\",\n",
        "    \"50pair\": \"50\",\n",
        "    \"80pair\": \"80\"\n",
        "}\n",
        "\n",
        "# Load exchange rates from CSV into a dictionary\n",
        "def load_exchange_rates():\n",
        "    exchange_rates_path = os.path.join(base_dir, \"exchange_rates\", \"xrate_april_2025.csv\")\n",
        "    rates_df = pd.read_csv(exchange_rates_path)\n",
        "    rates_dict = {}\n",
        "\n",
        "    if not rates_df.empty:\n",
        "        rate_row = rates_df.iloc[0]\n",
        "        for currency in rates_df.columns:\n",
        "            if currency != 'Date' and currency.strip():\n",
        "                try:\n",
        "                    usd_rate = float(rate_row.get(' USD', 1.0))\n",
        "                    currency_rate = float(rate_row.get(currency))\n",
        "                    rates_dict[currency.strip()] = currency_rate / usd_rate\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "        rates_dict['USD'] = 1.0\n",
        "\n",
        "    return rates_dict\n",
        "\n",
        "EXCHANGE_RATES = load_exchange_rates()\n",
        "PRICE_RANGE_TOLERANCE = 0.10  # 10% tolerance\n",
        "\n",
        "# Format product data with price normalization and range matching\n",
        "def serialize_product(entry, side):\n",
        "    fields = ['title', 'brand', 'description', 'cluster_id']\n",
        "    parts = []\n",
        "\n",
        "    # Handle price normalization and range matching\n",
        "    price = entry.get(f\"price_{side}\")\n",
        "    currency = entry.get(f\"priceCurrency_{side}\")\n",
        "\n",
        "    if price and price != \"null\" and currency and currency != \"null\":\n",
        "        try:\n",
        "            parts.append(f\"price: {price}\")\n",
        "            parts.append(f\"priceCurrency: {currency}\")\n",
        "\n",
        "            price_value = float(price)\n",
        "            conversion_rate = EXCHANGE_RATES.get(currency, 1.0)\n",
        "            normalized_price = price_value * conversion_rate\n",
        "\n",
        "            lower_bound = normalized_price * (1 - PRICE_RANGE_TOLERANCE)\n",
        "            upper_bound = normalized_price * (1 + PRICE_RANGE_TOLERANCE)\n",
        "\n",
        "            parts.append(f\"price_usd: {normalized_price:.2f}\")\n",
        "            parts.append(f\"price_range: {lower_bound:.2f}-{upper_bound:.2f} USD\")\n",
        "        except (ValueError, TypeError):\n",
        "            parts.append(f\"price: {price}\")\n",
        "            parts.append(f\"priceCurrency: {currency}\")\n",
        "\n",
        "    # Add other product fields\n",
        "    for field in fields:\n",
        "        key = f\"{field}_{side}\"\n",
        "        val = entry.get(key)\n",
        "        if val and val != \"null\":\n",
        "            parts.append(f\"{field}: {val}\")\n",
        "\n",
        "    return \" || \".join(parts)\n",
        "\n",
        "# Main processing loop: convert files across all data splits\n",
        "for folder_name, tag in splits.items():\n",
        "    folder_path = os.path.join(base_dir, folder_name)\n",
        "    for fname in os.listdir(folder_path):\n",
        "        if fname.endswith(\".json.gz\") and (\"train\" in fname or \"valid\" in fname):\n",
        "            input_path = os.path.join(folder_path, fname)\n",
        "            output_name = fname.replace(\".json.gz\", f\"_ditto_{tag}.jsonl\")\n",
        "            output_path = os.path.join(folder_path, output_name)\n",
        "\n",
        "            print(f\"Converting {fname} → {output_name}...\")\n",
        "            with gzip.open(input_path, 'rt', encoding='utf-8') as read, jsonlines.open(output_path, mode='w') as writer:\n",
        "                for line in read:\n",
        "                    raw = json.loads(line)\n",
        "                    writer.write({\n",
        "                        \"text_left\": serialize_product(raw, \"left\"),\n",
        "                        \"text_right\": serialize_product(raw, \"right\"),\n",
        "                        \"label\": str(raw[\"label\"]),\n",
        "                        \"cluster_id_left\": raw.get(\"cluster_id_left\", \"\"),\n",
        "                        \"cluster_id_right\": raw.get(\"cluster_id_right\", \"\")\n",
        "                    })\n",
        "            print(f\"Saved: {output_path}\")"
      ],
      "metadata": {
        "id": "1ZuV1BnHUQXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "oG1HNc88KoK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Convert .jsonl into dataframe\n",
        "\n",
        "import pandas as pd\n",
        "import jsonlines\n",
        "\n",
        "def jsonl_to_dataframe(file_path):\n",
        "  \"\"\"Converts a JSONL file to a Pandas DataFrame.\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the JSONL file.\n",
        "\n",
        "  Returns:\n",
        "    A Pandas DataFrame representing the data in the JSONL file.\n",
        "    Returns None if there's an error during file processing.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    data = []\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "      for obj in reader:\n",
        "        data.append(obj)\n",
        "    return pd.DataFrame(data)\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    return None\n",
        "\n",
        "# Example usage (assuming 'output_path' from your previous code):\n",
        "# Replace with your actual file path\n",
        "# Example file path (modify to your specific jsonl file)\n",
        "file_path = \"/content/drive/MyDrive/cs774/WDC/80pair/wdcproducts80cc20rnd000un_train_large_ditto_80.jsonl\"\n",
        "\n",
        "df = jsonl_to_dataframe(file_path)\n",
        "\n",
        "if df is not None:\n",
        "    print(df.head())\n",
        "    print(df.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJcO6qlxKrkP",
        "outputId": "6d54b602-8b47-4304-bfa9-976d340921b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           text_left  \\\n",
            "0  price: 119 || priceCurrency: EUR || price_usd:...   \n",
            "1  price: 12.48 || priceCurrency: GBP || price_us...   \n",
            "2  price: 296.39 || priceCurrency: EUR || price_u...   \n",
            "3  price: 69.90 || priceCurrency: EUR || price_us...   \n",
            "4  price: 4.0099E2 || priceCurrency: EUR || price...   \n",
            "\n",
            "                                          text_right label  cluster_id_left  \\\n",
            "0  price: 545.4008 || priceCurrency: RON || price...     0           156996   \n",
            "1  price: 24 || priceCurrency: EUR || price_usd: ...     1           373003   \n",
            "2  price: 234.99 || priceCurrency: CAD || price_u...     1             9046   \n",
            "3  price: 69.00 || priceCurrency: EUR || price_us...     1           643961   \n",
            "4  price: 25.95 || priceCurrency: EUR || price_us...     0            56526   \n",
            "\n",
            "   cluster_id_right  \n",
            "0            549556  \n",
            "1            373003  \n",
            "2              9046  \n",
            "3            643961  \n",
            "4            435008  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19835 entries, 0 to 19834\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   text_left         19835 non-null  object\n",
            " 1   text_right        19835 non-null  object\n",
            " 2   label             19835 non-null  object\n",
            " 3   cluster_id_left   19835 non-null  int64 \n",
            " 4   cluster_id_right  19835 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 774.9+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preview of the processed data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "axWfr04dSLdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preview_converted_data(file_path, num_samples=5):\n",
        "    print(f\"\\nPreviewing {num_samples} entries from converted data: {file_path}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    count = 0\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for entry in reader:\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "\n",
        "            print(f\"Entry #{count+1}:\")\n",
        "            print(f\"Left Product: {entry.get('text_left', 'N/A')}\")\n",
        "            print(f\"Right Product: {entry.get('text_right', 'N/A')}\")\n",
        "            print(f\"Match: {entry.get('label', 'N/A')}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            count += 1\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "WYV4RFIyalVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_folder = os.path.join(base_dir, list(splits.keys())[0])  # Use first split folder\n",
        "sample_files = [f for f in os.listdir(sample_folder) if f.endswith(\"_ditto_\" + splits[list(splits.keys())[0]] + \".jsonl\") and \"train\" in f]\n",
        "if sample_files:\n",
        "    preview_converted_data(os.path.join(sample_folder, sample_files[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k-4V1gIRuIJ",
        "outputId": "6e8565c5-70e8-4119-c78a-cd79023c0c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Previewing 5 entries from converted data: /content/drive/MyDrive/cs774/WDC/20pair/wdcproducts20cc80rnd000un_train_small_ditto_20.jsonl\n",
            "--------------------------------------------------------------------------------\n",
            "Entry #1:\n",
            "Left Product: title: Western Digital Blue SSD 3D 2TB 2.5 (WDS200T2B0A) || cluster_id: 1455368\n",
            "Right Product: price: 62.436001 || priceCurrency: EUR || price_usd: 62.44 || price_range: 56.19-68.68 USD || title: Western Digital Blue PC 2.5\\\" 250 GB Serial ATA III || description: 250 GB, 2.5\\\", SATA 6Gb/s, 540/500 MB/s || cluster_id: 483248\n",
            "Match: 0\n",
            "----------------------------------------\n",
            "Entry #2:\n",
            "Left Product: price: 39.99 || priceCurrency: USD || price_usd: 39.99 || price_range: 35.99-43.99 USD || title: Compatible Canon 120 Toner Cartridge || brand: Canon || description: Get your printing right the first time and every time. This eco-friendly cartridge is compatible with the Canon 120. You will get first-rate print quality with crisp vibrant output from the first page to thelast. We make all of our cartridges with the highest quality new and recycled components in ISO9001 / ISO14001 certified factories. They are easy to install, affordable and deliver sharp professional results every time you print. || cluster_id: 944768\n",
            "Right Product: price: 198.24 || priceCurrency: EUR || price_usd: 198.24 || price_range: 178.42-218.06 USD || title: MSI MPG Z490 GAMING EDGE WIFI Mainboard - Intel Z490 - Intel LGA1200 socket - DDR4 RAM - ATX || description: Mainboard, ATX, Intel LGA1200 Socket, Intel Z490, 2 x PCI-Express 3.0 x16, Dual DDR4-2933 - 4 x DIMM slots, 6 x SATA-600 / 2 x M.2 NVMe (2242 / 2260 / 2280), USB 3.2 Type A & C, DisplayPort / HDMI Verbindung, Realtek 8125B 2.5 Gigabit LAN, Intel WiFi 6 (802.11ax) & Bluetooth 5.1, Intel HD Graphics unterstützung (CPU erforderlich), Realtek ALC1220 HD Audio (8-Kanäle), stützen CrossFireX - MSI Mystic Light RGB support || cluster_id: 782402\n",
            "Match: 0\n",
            "----------------------------------------\n",
            "Entry #3:\n",
            "Left Product: price: 2421.68 || priceCurrency: TRY || price_usd: 93030.94 || price_range: 83727.84-102334.03 USD || title: Asus STRIX X570-F GAMING DDR4 S+GL AM4 (ATX) || brand: ASUS || description: PCI Express x1 Slot Says3Tmleik Grnt KartVarlemci reticisiAmdPCI Express x16 Slot Say3PCI Slot SaysYokMaksimum Bellek128 GBYap StandartATXGaranti Sresi36 AyTmleik A Kart10/100/1000 MbpsUyumlu lemcilerAmd Ryzen 9Uyumlu lemcilerAmd Ryzen 5Portlar1x USB Type-CPCI Express x4 Slot SayYokSabit Diskler2x M.2 SoketSabit Diskler8 x SATA III ( 6Gb/s )Soket YapsAmd Socket AM4Ram HzDDR4 2133 MHzRam HzDDR4 2400 MHz || cluster_id: 80156\n",
            "Right Product: price: 269.00 || priceCurrency: AUD || price_usd: 421.54 || price_range: 379.38-463.69 USD || title: Asus ROG STRIX B550-F GAMING AM4 ATX Motherboard || description: ASUS ROG STRIX B550-F GAMING AM4 ATX Motherboard - Memory Support: 4x DDR4 DIMM Slots (128GB, 5000 MHz O.C) - Expansion Slots: 1x PCIe 4.0 x16 (x16 mode), 1x PCIe 3.0 x16 (x4 mode) & 3x PCIe 3.0 x1 - Storage Support: 2x M.2 Slots & 6x SATA 6Gb/s Connectors - Network: Intel® I225-V 2.5Gb Ethernet - ROG SupremeFX 7.1-Channel High Definition Audio CODEC S1220A - Rear USB Ports: 2x USB 3.2 Gen 2 (1x USB-C), 4x USB 3.2 Gen 1 & 2x USB 2.0 - 3 Years Limited Warranty || cluster_id: 516888\n",
            "Match: 0\n",
            "----------------------------------------\n",
            "Entry #4:\n",
            "Left Product: price: 3028 || priceCurrency: PLN || price_usd: 11380.13 || price_range: 10242.12-12518.14 USD || title: Monitor 27 SW2700PT LED QHD,IPS,HDMI,DP,USB || cluster_id: 719046\n",
            "Right Product: price: 2192.23 || priceCurrency: PLN || price_usd: 8239.06 || price_range: 7415.15-9062.96 USD || title: Monitor 34 cali XUB3493WQSU-B1 IPS UWQHD DP/USB/2xHDMI || cluster_id: 133128\n",
            "Match: 0\n",
            "----------------------------------------\n",
            "Entry #5:\n",
            "Left Product: price: 81.32 || priceCurrency: OMR || price_usd: 81.32 || price_range: 73.19-89.45 USD || title: ASUS TUF X470 Plus Gaming with Aura Sync RGB LED lighting AM4 DDR4 HDMI DVI M.2 ATX Motherboard || description: AMD Ryzen 2 AM4 and 7th generation Athlon processors to maximize connectivity and speed with dual NVMe M.2, USB 3.1 Gen2 and gigabit LAN Military-grade TUF components like TUF LANGuard, TUF Chokes, TUF Capacitors, and TUF MOSFETs maximize durability Gamer’s Guardian with SafeSlot and Fan Xpert 4 Core provides hardware-level safeguards for maximum performance with dynamic system cooling Unmatched personalization with ASUS exclusive Aura Sync RGB lighting and additional RGB header 3-Year Warranty for guaranteed reliability built on military-grade engineering || cluster_id: 939822\n",
            "Right Product: price: 255.07 || priceCurrency: EUR || price_usd: 255.07 || price_range: 229.56-280.58 USD || title: Evolve 80 MS Stereo Binaural Head-band Black || description: Jabra Evolve 80 MS Stereo. Product type: Headset, Wearing style: Head-band, Product colour: Black. Connectivity technology: Wired. Ear coupling: Supraaural. Weight: 324 g. Number of products included: 1 pc(s), Interface type: 3.5mm / USBTV || cluster_id: 192208\n",
            "Match: 0\n",
            "----------------------------------------\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prelimnary Rule Based Matching**"
      ],
      "metadata": {
        "id": "P5S7PP6KvHIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def string_similarity(a, b):\n",
        "    if not a or not b:\n",
        "        return 0\n",
        "    return SequenceMatcher(None, str(a).lower(), str(b).lower()).ratio()\n",
        "\n",
        "def jaccard_similarity(a, b):\n",
        "    if not a or not b:\n",
        "        return 0\n",
        "    a_tokens = set(re.findall(r'\\w+', str(a).lower()))\n",
        "    b_tokens = set(re.findall(r'\\w+', str(b).lower()))\n",
        "\n",
        "    intersection = len(a_tokens.intersection(b_tokens))\n",
        "    union = len(a_tokens.union(b_tokens))\n",
        "\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "def evaluate_similarity_metrics(file_path):\n",
        "    \"\"\"Evaluate different similarity metrics against provided labels\"\"\"\n",
        "    true_labels = []\n",
        "    title_sims = []\n",
        "    brand_sims = []\n",
        "    desc_sims = []\n",
        "    combined_sims = []  # Weighted combination\n",
        "\n",
        "    # Define weights for combined score\n",
        "    weights = {\n",
        "        'title': 0.5,\n",
        "        'brand': 0.3,\n",
        "        'desc': 0.2\n",
        "    }\n",
        "\n",
        "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            raw = json.loads(line)\n",
        "\n",
        "            # Get true label\n",
        "            true_label = int(raw.get(\"label\", 0))\n",
        "            true_labels.append(true_label)\n",
        "\n",
        "            # Calculate similarity scores\n",
        "            title_sim = string_similarity(raw.get(\"title_left\"), raw.get(\"title_right\"))\n",
        "            brand_sim = string_similarity(raw.get(\"brand_left\"), raw.get(\"brand_right\"))\n",
        "            desc_sim = jaccard_similarity(raw.get(\"description_left\"), raw.get(\"description_right\"))\n",
        "\n",
        "            # Store individual similarities\n",
        "            title_sims.append(title_sim)\n",
        "            brand_sims.append(brand_sim)\n",
        "            desc_sims.append(desc_sim)\n",
        "\n",
        "            # Compute combined score\n",
        "            combined_sim = (weights['title'] * title_sim +\n",
        "                           weights['brand'] * brand_sim +\n",
        "                           weights['desc'] * desc_sim)\n",
        "            combined_sims.append(combined_sim)\n",
        "\n",
        "    # Test different thresholds for each metric\n",
        "    metrics = {\n",
        "        'Title Similarity': title_sims,\n",
        "        'Brand Similarity': brand_sims,\n",
        "        'Description Similarity': desc_sims,\n",
        "        'Combined Similarity': combined_sims\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        best_f1 = 0\n",
        "        best_threshold = 0\n",
        "        best_report = None\n",
        "\n",
        "        # Try different thresholds\n",
        "        for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "            predictions = [1 if score >= threshold else 0 for score in scores]\n",
        "            report = classification_report(true_labels, predictions, output_dict=True, zero_division=0)\n",
        "            f1 = report['1']['f1-score']  # F1 for the match class\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "                best_report = report\n",
        "\n",
        "        results[name] = {\n",
        "            'threshold': best_threshold,\n",
        "            'f1': best_f1,\n",
        "            'precision': best_report['1']['precision'],\n",
        "            'recall': best_report['1']['recall']\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run evaluation on validation data\n",
        "for folder_name, tag in splits.items():\n",
        "    folder_path = os.path.join(base_dir, folder_name)\n",
        "    validation_files = [f for f in os.listdir(folder_path) if f.endswith(\".json.gz\") and \"valid\" in f]\n",
        "\n",
        "    if validation_files:\n",
        "        print(f\"\\nEvaluating similarity metrics on {folder_name} split\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for val_file in validation_files:\n",
        "            file_path = os.path.join(folder_path, val_file)\n",
        "            print(f\"File: {val_file}\")\n",
        "\n",
        "            results = evaluate_similarity_metrics(file_path)\n",
        "\n",
        "            # Print results table\n",
        "            print(f\"{'Metric':<25} {'Threshold':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
        "            print(\"-\" * 65)\n",
        "\n",
        "            for metric, stats in results.items():\n",
        "                print(f\"{metric:<25} {stats['threshold']:<10.2f} {stats['f1']:<10.4f} {stats['precision']:<10.4f} {stats['recall']:<10.4f}\")\n",
        "\n",
        "            print()"
      ],
      "metadata": {
        "id": "dntL03VcSWQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf858e5-8c72-43da-c9fb-c313437f9aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating similarity metrics on 20pair split\n",
            "------------------------------------------------------------\n",
            "File: wdcproducts20cc80rnd000un_valid_large.json.gz\n",
            "Metric                    Threshold  F1         Precision  Recall    \n",
            "-----------------------------------------------------------------\n",
            "Title Similarity          0.40       0.3133     0.1959     0.7820    \n",
            "Brand Similarity          0.50       0.1441     0.1850     0.1180    \n",
            "Description Similarity    0.10       0.2651     0.2388     0.2980    \n",
            "Combined Similarity       0.20       0.3079     0.1882     0.8460    \n",
            "\n",
            "File: wdcproducts20cc80rnd000un_valid_medium.json.gz\n",
            "Metric                    Threshold  F1         Precision  Recall    \n",
            "-----------------------------------------------------------------\n",
            "Title Similarity          0.40       0.3752     0.2468     0.7820    \n",
            "Brand Similarity          0.50       0.1553     0.2269     0.1180    \n",
            "Description Similarity    0.10       0.2998     0.3016     0.2980    \n",
            "Combined Similarity       0.20       0.3709     0.2375     0.8460    \n",
            "\n",
            "File: wdcproducts20cc80rnd000un_valid_small.json.gz\n",
            "Metric                    Threshold  F1         Precision  Recall    \n",
            "-----------------------------------------------------------------\n",
            "Title Similarity          0.40       0.5055     0.3734     0.7820    \n",
            "Brand Similarity          0.50       0.1743     0.3333     0.1180    \n",
            "Description Similarity    0.10       0.3465     0.4139     0.2980    \n",
            "Combined Similarity       0.20       0.5021     0.3570     0.8460    \n",
            "\n",
            "\n",
            "Evaluating similarity metrics on 50pair split\n",
            "------------------------------------------------------------\n",
            "File: wdcproducts50cc50rnd000un_valid_large.json.gz\n",
            "Metric                    Threshold  F1         Precision  Recall    \n",
            "-----------------------------------------------------------------\n",
            "Title Similarity          0.40       0.3021     0.1891     0.7500    \n",
            "Brand Similarity          0.50       0.1477     0.1785     0.1260    \n",
            "Description Similarity    0.10       0.2704     0.2352     0.3180    \n",
            "Combined Similarity       0.20       0.2992     0.1829     0.8220    \n",
            "\n",
            "File: wdcproducts50cc50rnd000un_valid_small.json.gz\n",
            "Metric                    Threshold  F1         Precision  Recall    \n",
            "-----------------------------------------------------------------\n",
            "Title Similarity          0.40       0.4918     0.3659     0.7500    \n",
            "Brand Similarity          0.50       0.1867     0.3600     0.1260    \n",
            "Description Similarity    0.10       0.3614     0.4184     0.3180    \n",
            "Combined Similarity       0.20       0.5015     0.3608     0.8220    \n",
            "\n",
            "File: wdcproducts50cc50rnd000un_valid_medium.json.gz\n",
            "Metric                    Threshold  F1         Precision  Recall    \n",
            "-----------------------------------------------------------------\n",
            "Title Similarity          0.40       0.3687     0.2445     0.7500    \n",
            "Brand Similarity          0.50       0.1622     0.2274     0.1260    \n",
            "Description Similarity    0.10       0.3052     0.2934     0.3180    \n",
            "Combined Similarity       0.20       0.3711     0.2397     0.8220    \n",
            "\n",
            "\n",
            "Evaluating similarity metrics on 80pair split\n",
            "------------------------------------------------------------\n",
            "File: wdcproducts80cc20rnd000un_valid_large.json.gz\n",
            "Metric                    Threshold  F1         Precision  Recall    \n",
            "-----------------------------------------------------------------\n",
            "Title Similarity          0.40       0.2846     0.1749     0.7640    \n",
            "Brand Similarity          0.50       0.1269     0.1499     0.1100    \n",
            "Description Similarity    0.10       0.2191     0.1872     0.2640    \n",
            "Combined Similarity       0.20       0.2846     0.1718     0.8300    \n",
            "\n",
            "File: wdcproducts80cc20rnd000un_valid_medium.json.gz\n",
            "Metric                    Threshold  F1         Precision  Recall    \n",
            "-----------------------------------------------------------------\n",
            "Title Similarity          0.30       0.3537     0.2181     0.9340    \n",
            "Brand Similarity          0.50       0.1442     0.2091     0.1100    \n",
            "Description Similarity    0.10       0.2551     0.2467     0.2640    \n",
            "Combined Similarity       0.20       0.3565     0.2270     0.8300    \n",
            "\n",
            "File: wdcproducts80cc20rnd000un_valid_small.json.gz\n",
            "Metric                    Threshold  F1         Precision  Recall    \n",
            "-----------------------------------------------------------------\n",
            "Title Similarity          0.30       0.4995     0.3409     0.9340    \n",
            "Brand Similarity          0.50       0.1615     0.3039     0.1100    \n",
            "Description Similarity    0.10       0.3038     0.3577     0.2640    \n",
            "Combined Similarity       0.20       0.4888     0.3464     0.8300    \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule Based Matching Analysis  \n",
        "\n",
        "Based on the similarity metrics evaluation across different data splits, several key patterns emerge. Across all datasets, title similarity and combined similarity consistently outperform brand and description similarity in terms of F1 score. Title similarity achieves F1 scores ranging from 0.28 to 0.50 at thresholds of 0.30-0.40, while the combined similarity approach shows comparable or slightly better performance at a lower threshold of 0.20. This suggests that product titles contain the most discriminative information for matching products.\n",
        "\n",
        "\n",
        "There's a clear precision-recall tradeoff visible in the results. Combined similarity consistently achieves the highest recall (82-84%) but at the cost of lower precision (17-36%), indicating it captures most matches but includes many false positives. Meanwhile, description similarity often has better precision than title similarity, especially in the small dataset splits, though its recall is significantly lower (26-32%). Interestingly, performance is consistently better on \"small\" dataset splits compared to \"medium\" and \"large\" splits across all metrics, suggesting that smaller datasets may contain more obvious matches or less diverse product descriptions.\n",
        "\n",
        "\n",
        "Brand similarity shows the weakest overall performance, with low recall (11-12%) indicating that many matching products either have different brand representations or missing brand information. The consistency of optimal thresholds across different splits (0.40 for title, 0.50 for brand, 0.10 for description, and 0.20 for combined similarity) provides a solid foundation for setting rule-based matching parameters.\n",
        "\n",
        "\n",
        "\n",
        "*   For title similarity (0.40), this moderate threshold balances precision and recall effectively. Titles often contain the core product information but may vary in format, additional descriptors, or word order. A threshold of 0.40 accommodates these variations while still requiring substantial similarity, capturing meaningful matches without being too restrictive. This threshold consistently delivers strong F1 scores across datasets.\n",
        "* Brand similarity's higher threshold (0.50) reflects that brands should match more precisely when present. Brands are typically shorter strings with standardized representations, so legitimate matches should show higher similarity. The stricter threshold helps avoid false positives from slightly different brand names that actually represent different manufacturers. Despite this higher threshold, brand similarity still shows lower overall performance, suggesting inconsistent brand representation in the dataset.\n",
        "* Description similarity's low threshold (0.10) acknowledges the inherent variability in product descriptions. Descriptions often contain the same core information but expressed differently, with varying levels of detail, word choices, and formatting. The 0.10 threshold captures cases where descriptions share some key terminology while accommodating this natural variation. Setting a higher threshold would miss too many legitimate matches given how differently the same product can be described.\n",
        "* The combined similarity threshold (0.20) leverages the weighted combination of all three metrics, allowing strong performance in one area to compensate for weaker matches in others. This explains why it achieves high recall while maintaining reasonable F1 scores - it can identify matches where only some aspects of the products align strongly.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ORvV418vxUaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Import your similarity functions from the artifacts I created earlier\n",
        "# You'll need to copy these functions into a module file or include them directly\n",
        "\n",
        "def string_similarity(a, b):\n",
        "    if not a or not b:\n",
        "        return 0\n",
        "    return SequenceMatcher(None, str(a).lower(), str(b).lower()).ratio()\n",
        "\n",
        "def jaccard_similarity(a, b):\n",
        "    if not a or not b:\n",
        "        return 0\n",
        "    a_tokens = set(re.findall(r'\\w+', str(a).lower()))\n",
        "    b_tokens = set(re.findall(r'\\w+', str(b).lower()))\n",
        "\n",
        "    intersection = len(a_tokens.intersection(b_tokens))\n",
        "    union = len(a_tokens.union(b_tokens))\n",
        "\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "def tokenize(text):\n",
        "    if not text:\n",
        "        return []\n",
        "    return re.findall(r'\\w+', str(text).lower())\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    if not a or not b:\n",
        "        return 0\n",
        "\n",
        "    a_tokens = tokenize(a)\n",
        "    b_tokens = tokenize(b)\n",
        "\n",
        "    if not a_tokens or not b_tokens:\n",
        "        return 0\n",
        "\n",
        "    vocabulary = set(a_tokens + b_tokens)\n",
        "\n",
        "    a_vector = [a_tokens.count(word) for word in vocabulary]\n",
        "    b_vector = [b_tokens.count(word) for word in vocabulary]\n",
        "\n",
        "    a_array = np.array(a_vector)\n",
        "    b_array = np.array(b_vector)\n",
        "\n",
        "    dot_product = np.dot(a_array, b_array)\n",
        "    norm_a = np.linalg.norm(a_array)\n",
        "    norm_b = np.linalg.norm(b_array)\n",
        "\n",
        "    if norm_a == 0 or norm_b == 0:\n",
        "        return 0\n",
        "\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "def evaluate_similarity_metrics(file_path):\n",
        "    \"\"\"Evaluate different similarity metrics against provided labels\"\"\"\n",
        "    # Implementation from the artifact above\n",
        "    # ...\n",
        "\n",
        "    # Simplified version for brevity\n",
        "    true_labels = []\n",
        "    title_sims = []\n",
        "    brand_sims = []\n",
        "    desc_sims = []\n",
        "    title_cosine_sims = []\n",
        "    brand_cosine_sims = []\n",
        "    desc_cosine_sims = []\n",
        "    combined_sims = []\n",
        "    cosine_combined_sims = []\n",
        "\n",
        "    weights = {\n",
        "        'title': 0.5,\n",
        "        'brand': 0.3,\n",
        "        'desc': 0.2\n",
        "    }\n",
        "\n",
        "    total_records = 0\n",
        "    match_count = 0\n",
        "\n",
        "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            raw = json.loads(line)\n",
        "            total_records += 1\n",
        "\n",
        "            true_label = int(raw.get(\"label\", 0))\n",
        "            true_labels.append(true_label)\n",
        "\n",
        "            if true_label == 1:\n",
        "                match_count += 1\n",
        "\n",
        "            # Extract field values\n",
        "            title_left = raw.get(\"title_left\", \"\")\n",
        "            title_right = raw.get(\"title_right\", \"\")\n",
        "            brand_left = raw.get(\"brand_left\", \"\")\n",
        "            brand_right = raw.get(\"brand_right\", \"\")\n",
        "            desc_left = raw.get(\"description_left\", \"\")\n",
        "            desc_right = raw.get(\"description_right\", \"\")\n",
        "\n",
        "            # Calculate similarities\n",
        "            title_sim = string_similarity(title_left, title_right)\n",
        "            brand_sim = string_similarity(brand_left, brand_right)\n",
        "            desc_sim = jaccard_similarity(desc_left, desc_right)\n",
        "\n",
        "            title_cosine_sim = cosine_similarity(title_left, title_right)\n",
        "            brand_cosine_sim = cosine_similarity(brand_left, brand_right)\n",
        "            desc_cosine_sim = cosine_similarity(desc_left, desc_right)\n",
        "\n",
        "            # Store similarities\n",
        "            title_sims.append(title_sim)\n",
        "            brand_sims.append(brand_sim)\n",
        "            desc_sims.append(desc_sim)\n",
        "            title_cosine_sims.append(title_cosine_sim)\n",
        "            brand_cosine_sims.append(brand_cosine_sim)\n",
        "            desc_cosine_sims.append(desc_cosine_sim)\n",
        "\n",
        "            # Compute combined scores\n",
        "            combined_sim = (weights['title'] * title_sim +\n",
        "                           weights['brand'] * brand_sim +\n",
        "                           weights['desc'] * desc_sim)\n",
        "            combined_sims.append(combined_sim)\n",
        "\n",
        "            cosine_combined_sim = (weights['title'] * title_cosine_sim +\n",
        "                                  weights['brand'] * brand_cosine_sim +\n",
        "                                  weights['desc'] * desc_cosine_sim)\n",
        "            cosine_combined_sims.append(cosine_combined_sim)\n",
        "\n",
        "    # Define all metrics to evaluate\n",
        "    metrics = {\n",
        "        'Title (String Similarity)': title_sims,\n",
        "        'Title (Cosine Similarity)': title_cosine_sims,\n",
        "        'Brand (String Similarity)': brand_sims,\n",
        "        'Brand (Cosine Similarity)': brand_cosine_sims,\n",
        "        'Description (Jaccard)': desc_sims,\n",
        "        'Description (Cosine)': desc_cosine_sims,\n",
        "        'Combined Original': combined_sims,\n",
        "        'Combined Cosine': cosine_combined_sims\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(f\"Total records: {total_records}, Matches: {match_count}, Non-matches: {total_records - match_count}\")\n",
        "    print(f\"Match rate: {match_count/total_records:.2%}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        best_f1 = 0\n",
        "        best_threshold = 0\n",
        "        best_report = None\n",
        "        best_confusion = None\n",
        "\n",
        "        # Try different thresholds\n",
        "        for threshold in np.arange(0.1, 1.0, 0.05):\n",
        "            predictions = [1 if score >= threshold else 0 for score in scores]\n",
        "            report = classification_report(true_labels, predictions, output_dict=True,)\n",
        "            conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "            if '1' in report:\n",
        "                f1 = report['1']['f1-score']\n",
        "            else:\n",
        "                f1 = 0\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "                best_report = report\n",
        "                best_confusion = conf_matrix\n",
        "\n",
        "        if best_report and '1' in best_report:\n",
        "            results[name] = {\n",
        "                'threshold': best_threshold,\n",
        "                'f1': best_f1,\n",
        "                'precision': best_report['1']['precision'],\n",
        "                'recall': best_report['1']['recall'],\n",
        "                'confusion_matrix': best_confusion\n",
        "            }\n",
        "        else:\n",
        "            results[name] = {\n",
        "                'threshold': best_threshold,\n",
        "                'f1': 0,\n",
        "                'precision': 0,\n",
        "                'recall': 0,\n",
        "                'confusion_matrix': best_confusion if best_confusion is not None else np.zeros((2, 2))\n",
        "            }\n",
        "\n",
        "    return results, true_labels, metrics\n",
        "\n",
        "# Add any additional helper functions from the artifacts\n",
        "# ...\n",
        "\n",
        "# Main function to execute\n",
        "def main():\n",
        "    # Set base directory from your data processing code\n",
        "    base_dir = \"/content/drive/MyDrive/cs774/WDC\"\n",
        "\n",
        "    # Create output directory\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_base_dir = os.path.join(base_dir, f\"similarity_evaluation_{timestamp}\")\n",
        "    os.makedirs(output_base_dir, exist_ok=True)\n",
        "\n",
        "    # Define splits to evaluate\n",
        "    splits = {\n",
        "        \"20pair\": \"20\",\n",
        "        \"50pair\": \"50\",\n",
        "        \"80pair\": \"80\"\n",
        "    }\n",
        "\n",
        "    # Store all results for comparison\n",
        "    all_split_results = {}\n",
        "\n",
        "    # Process each split\n",
        "    for folder_name, tag in splits.items():\n",
        "        print(f\"\\n\\n{'='*80}\")\n",
        "        print(f\"Processing {folder_name} split\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        folder_path = os.path.join(base_dir, folder_name)\n",
        "        output_dir = os.path.join(output_base_dir, folder_name)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Find all validation files for this split\n",
        "        validation_files = [f for f in os.listdir(folder_path)\n",
        "                           if f.endswith(\".json.gz\") and \"valid\" in f]\n",
        "\n",
        "        split_results = {}\n",
        "\n",
        "        for val_file in validation_files:\n",
        "            file_path = os.path.join(folder_path, val_file)\n",
        "            file_output_dir = os.path.join(output_dir, os.path.splitext(val_file)[0])\n",
        "            os.makedirs(file_output_dir, exist_ok=True)\n",
        "\n",
        "            print(f\"\\nEvaluating file: {val_file}\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "            # Time the evaluation\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Run evaluation\n",
        "            results, true_labels, metrics_dict = evaluate_similarity_metrics(file_path)\n",
        "\n",
        "            # Create results DataFrame\n",
        "            metrics_df = pd.DataFrame([\n",
        "                {\n",
        "                    'Metric': metric,\n",
        "                    'Threshold': stats['threshold'],\n",
        "                    'F1': stats['f1'],\n",
        "                    'Precision': stats['precision'],\n",
        "                    'Recall': stats['recall']\n",
        "                }\n",
        "                for metric, stats in results.items()\n",
        "            ])\n",
        "\n",
        "            # Sort by F1 score\n",
        "            metrics_df = metrics_df.sort_values('F1', ascending=False)\n",
        "\n",
        "            # Print results\n",
        "            print(metrics_df.to_string(index=False))\n",
        "\n",
        "            # Save results to CSV\n",
        "            metrics_df.to_csv(os.path.join(file_output_dir, 'metrics_comparison.csv'), index=False)\n",
        "\n",
        "            # Generate plots\n",
        "            # F1 comparison\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            ax = sns.barplot(x='Metric', y='F1', data=metrics_df)\n",
        "            plt.title(f'F1 Score Comparison - {val_file}')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.tight_layout()\n",
        "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "            # Add value labels\n",
        "            for i, bar in enumerate(ax.patches):\n",
        "                ax.text(\n",
        "                    bar.get_x() + bar.get_width()/2.,\n",
        "                    bar.get_height() + 0.01,\n",
        "                    f'{bar.get_height():.3f}',\n",
        "                    ha='center',\n",
        "                    fontsize=9\n",
        "                )\n",
        "\n",
        "            plt.savefig(os.path.join(file_output_dir, 'f1_comparison.png'))\n",
        "            plt.close()\n",
        "\n",
        "            # Log execution time\n",
        "            execution_time = time.time() - start_time\n",
        "            print(f\"Evaluation completed in {execution_time:.2f} seconds\")\n",
        "\n",
        "            # Store results for this file\n",
        "            split_results[val_file] = metrics_df\n",
        "\n",
        "        # Combine results for this split\n",
        "        if split_results:\n",
        "            combined_df = pd.DataFrame()\n",
        "\n",
        "            for file_name, df in split_results.items():\n",
        "                temp_df = df.copy()\n",
        "                temp_df['File'] = file_name\n",
        "                combined_df = pd.concat([combined_df, temp_df])\n",
        "\n",
        "            # Save combined results for this split\n",
        "            combined_df.to_csv(os.path.join(output_dir, 'all_files_comparison.csv'), index=False)\n",
        "\n",
        "            # Create aggregate metrics for this split\n",
        "            agg_df = combined_df.groupby('Metric').agg({\n",
        "                'F1': 'mean',\n",
        "                'Precision': 'mean',\n",
        "                'Recall': 'mean',\n",
        "                'Threshold': 'mean'\n",
        "            }).reset_index()\n",
        "\n",
        "            agg_df = agg_df.sort_values('F1', ascending=False)\n",
        "            agg_df.to_csv(os.path.join(output_dir, 'aggregate_metrics.csv'), index=False)\n",
        "\n",
        "            # Store results for overall comparison\n",
        "            all_split_results[folder_name] = agg_df\n",
        "\n",
        "    # Create overall comparison across all splits\n",
        "    if all_split_results:\n",
        "        print(\"\\n\\nGenerating comparison across all splits...\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Create a comparison table\n",
        "        comparison_data = []\n",
        "\n",
        "        for split_name, df in all_split_results.items():\n",
        "            for _, row in df.iterrows():\n",
        "                comparison_data.append({\n",
        "                    'Split': split_name,\n",
        "                    'Metric': row['Metric'],\n",
        "                    'F1': row['F1'],\n",
        "                    'Precision': row['Precision'],\n",
        "                    'Recall': row['Recall'],\n",
        "                    'Threshold': row['Threshold']\n",
        "                })\n",
        "\n",
        "        comparison_df = pd.DataFrame(comparison_data)\n",
        "        comparison_df.to_csv(os.path.join(output_base_dir, 'all_splits_comparison.csv'), index=False)\n",
        "\n",
        "        # Create a pivot table for easier comparison\n",
        "        pivot_df = comparison_df.pivot_table(\n",
        "            index='Metric',\n",
        "            columns='Split',\n",
        "            values=['F1', 'Precision', 'Recall'],\n",
        "            aggfunc='mean'\n",
        "        )\n",
        "\n",
        "        pivot_df.to_csv(os.path.join(output_base_dir, 'metrics_by_split.csv'))\n",
        "\n",
        "        # Plot comparison of F1 scores across splits\n",
        "        plt.figure(figsize=(14, 8))\n",
        "\n",
        "        # Reshape data for plotting\n",
        "        f1_comparison = comparison_df.pivot(index='Metric', columns='Split', values='F1')\n",
        "\n",
        "        # Plot\n",
        "        ax = f1_comparison.plot(kind='bar', figsize=(14, 8))\n",
        "        plt.title('F1 Score Comparison Across Different Splits')\n",
        "        plt.xlabel('Metric')\n",
        "        plt.ylabel('F1 Score')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.legend(title='Split')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig(os.path.join(output_base_dir, 'f1_comparison_across_splits.png'))\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"All evaluation results saved to: {output_base_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "itYfz6EBQO-R",
        "outputId": "7151ccc8-11c6-477a-8595-445d1ceffd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "================================================================================\n",
            "Processing 20pair split\n",
            "================================================================================\n",
            "\n",
            "Evaluating file: wdcproducts20cc80rnd000un_valid_large.json.gz\n",
            "----------------------------------------------------------------------\n",
            "Total records: 4500, Matches: 500, Non-matches: 4000\n",
            "Match rate: 11.11%\n",
            "----------------------------------------------------------------------\n",
            "                   Metric  Threshold       F1  Precision  Recall\n",
            "Title (Cosine Similarity)       0.55 0.424242   0.382637   0.476\n",
            "          Combined Cosine       0.30 0.400564   0.309368   0.568\n",
            "        Combined Original       0.25 0.318813   0.209566   0.666\n",
            "Title (String Similarity)       0.40 0.315491   0.197864   0.778\n",
            "    Description (Jaccard)       0.10 0.265125   0.238782   0.298\n",
            "     Description (Cosine)       0.25 0.253943   0.209635   0.322\n",
            "Brand (String Similarity)       0.45 0.143552   0.183230   0.118\n",
            "Brand (Cosine Similarity)       0.50 0.136986   0.181518   0.110\n",
            "Evaluation completed in 7.01 seconds\n",
            "\n",
            "Evaluating file: wdcproducts20cc80rnd000un_valid_medium.json.gz\n",
            "----------------------------------------------------------------------\n",
            "Total records: 3500, Matches: 500, Non-matches: 3000\n",
            "Match rate: 14.29%\n",
            "----------------------------------------------------------------------\n",
            "                   Metric  Threshold       F1  Precision  Recall\n",
            "Title (Cosine Similarity)       0.50 0.443077   0.360000   0.576\n",
            "          Combined Cosine       0.30 0.432927   0.349754   0.568\n",
            "Title (String Similarity)       0.40 0.377487   0.249199   0.778\n",
            "        Combined Original       0.25 0.373318   0.259346   0.666\n",
            "    Description (Jaccard)       0.10 0.299799   0.301619   0.298\n",
            "     Description (Cosine)       0.25 0.286477   0.258013   0.322\n",
            "Brand (String Similarity)       0.45 0.154856   0.225191   0.118\n",
            "Brand (Cosine Similarity)       0.50 0.147256   0.222672   0.110\n",
            "Evaluation completed in 4.63 seconds\n",
            "\n",
            "Evaluating file: wdcproducts20cc80rnd000un_valid_small.json.gz\n",
            "----------------------------------------------------------------------\n",
            "Total records: 2500, Matches: 500, Non-matches: 2000\n",
            "Match rate: 20.00%\n",
            "----------------------------------------------------------------------\n",
            "                   Metric  Threshold       F1  Precision  Recall\n",
            "Title (Cosine Similarity)       0.25 0.541642   0.382087   0.930\n",
            "          Combined Cosine       0.15 0.538194   0.378664   0.930\n",
            "Title (String Similarity)       0.35 0.512669   0.363409   0.870\n",
            "        Combined Original       0.20 0.502973   0.357868   0.846\n",
            "     Description (Cosine)       0.15 0.353150   0.317384   0.398\n",
            "    Description (Jaccard)       0.10 0.346512   0.413889   0.298\n",
            "Brand (String Similarity)       0.45 0.173275   0.325967   0.118\n",
            "Brand (Cosine Similarity)       0.10 0.164425   0.325444   0.110\n",
            "Evaluation completed in 5.38 seconds\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Processing 50pair split\n",
            "================================================================================\n",
            "\n",
            "Evaluating file: wdcproducts50cc50rnd000un_valid_large.json.gz\n",
            "----------------------------------------------------------------------\n",
            "Total records: 4500, Matches: 500, Non-matches: 4000\n",
            "Match rate: 11.11%\n",
            "----------------------------------------------------------------------\n",
            "                   Metric  Threshold       F1  Precision  Recall\n",
            "Title (Cosine Similarity)       0.55 0.369876   0.353370   0.388\n",
            "          Combined Cosine       0.25 0.354497   0.241007   0.670\n",
            "Title (String Similarity)       0.45 0.309603   0.203390   0.648\n",
            "        Combined Original       0.25 0.305395   0.201142   0.634\n",
            "    Description (Jaccard)       0.10 0.270408   0.235207   0.318\n",
            "     Description (Cosine)       0.25 0.263836   0.212454   0.348\n",
            "Brand (String Similarity)       0.45 0.146682   0.175487   0.126\n",
            "Brand (Cosine Similarity)       0.10 0.145065   0.178886   0.122\n",
            "Evaluation completed in 5.13 seconds\n",
            "\n",
            "Evaluating file: wdcproducts50cc50rnd000un_valid_small.json.gz\n",
            "----------------------------------------------------------------------\n",
            "Total records: 2500, Matches: 500, Non-matches: 2000\n",
            "Match rate: 20.00%\n",
            "----------------------------------------------------------------------\n",
            "                   Metric  Threshold       F1  Precision  Recall\n",
            "Title (Cosine Similarity)       0.20 0.546689   0.381215   0.966\n",
            "          Combined Cosine       0.10 0.542429   0.375288   0.978\n",
            "Title (String Similarity)       0.35 0.505761   0.362924   0.834\n",
            "        Combined Original       0.20 0.501225   0.361307   0.818\n",
            "     Description (Cosine)       0.10 0.370709   0.299630   0.486\n",
            "    Description (Jaccard)       0.10 0.361364   0.418421   0.318\n",
            "Brand (String Similarity)       0.45 0.186115   0.355932   0.126\n",
            "Brand (Cosine Similarity)       0.10 0.182090   0.358824   0.122\n",
            "Evaluation completed in 3.52 seconds\n",
            "\n",
            "Evaluating file: wdcproducts50cc50rnd000un_valid_medium.json.gz\n",
            "----------------------------------------------------------------------\n",
            "Total records: 3500, Matches: 500, Non-matches: 3000\n",
            "Match rate: 14.29%\n",
            "----------------------------------------------------------------------\n",
            "                   Metric  Threshold       F1  Precision  Recall\n",
            "          Combined Cosine       0.25 0.406553   0.291812   0.670\n",
            "Title (Cosine Similarity)       0.45 0.398098   0.301440   0.586\n",
            "        Combined Original       0.20 0.370975   0.239883   0.818\n",
            "Title (String Similarity)       0.40 0.367611   0.244547   0.740\n",
            "    Description (Jaccard)       0.10 0.305182   0.293358   0.318\n",
            "     Description (Cosine)       0.25 0.302346   0.267281   0.348\n",
            "Brand (String Similarity)       0.45 0.161125   0.223404   0.126\n",
            "Brand (Cosine Similarity)       0.10 0.159061   0.228464   0.122\n",
            "Evaluation completed in 5.96 seconds\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Processing 80pair split\n",
            "================================================================================\n",
            "\n",
            "Evaluating file: wdcproducts80cc20rnd000un_valid_large.json.gz\n",
            "----------------------------------------------------------------------\n",
            "Total records: 4500, Matches: 500, Non-matches: 4000\n",
            "Match rate: 11.11%\n",
            "----------------------------------------------------------------------\n",
            "                   Metric  Threshold       F1  Precision  Recall\n",
            "          Combined Cosine       0.20 0.300307   0.185837   0.782\n",
            "Title (Cosine Similarity)       0.40 0.294803   0.189954   0.658\n",
            "        Combined Original       0.20 0.285026   0.172141   0.828\n",
            "Title (String Similarity)       0.40 0.284748   0.175301   0.758\n",
            "    Description (Jaccard)       0.10 0.219087   0.187234   0.264\n",
            "     Description (Cosine)       0.25 0.217160   0.175031   0.286\n",
            "Brand (String Similarity)       0.50 0.127462   0.151515   0.110\n",
            "Brand (Cosine Similarity)       0.10 0.127021   0.150273   0.110\n",
            "Evaluation completed in 5.14 seconds\n",
            "\n",
            "Evaluating file: wdcproducts80cc20rnd000un_valid_medium.json.gz\n",
            "----------------------------------------------------------------------\n",
            "Total records: 3500, Matches: 500, Non-matches: 3000\n",
            "Match rate: 14.29%\n",
            "----------------------------------------------------------------------\n",
            "                   Metric  Threshold       F1  Precision  Recall\n",
            "          Combined Cosine       0.10 0.370572   0.230063   0.952\n",
            "Title (Cosine Similarity)       0.15 0.369800   0.229008   0.960\n",
            "Title (String Similarity)       0.35 0.358953   0.227516   0.850\n",
            "        Combined Original       0.20 0.356436   0.227098   0.828\n",
            "     Description (Cosine)       0.10 0.261343   0.187337   0.432\n",
            "    Description (Jaccard)       0.10 0.255072   0.246729   0.264\n",
            "Brand (String Similarity)       0.50 0.144928   0.212355   0.110\n",
            "Brand (Cosine Similarity)       0.10 0.144357   0.209924   0.110\n",
            "Evaluation completed in 4.33 seconds\n",
            "\n",
            "Evaluating file: wdcproducts80cc20rnd000un_valid_small.json.gz\n",
            "----------------------------------------------------------------------\n",
            "Total records: 2500, Matches: 500, Non-matches: 2000\n",
            "Match rate: 20.00%\n",
            "----------------------------------------------------------------------\n",
            "                   Metric  Threshold       F1  Precision  Recall\n",
            "          Combined Cosine       0.10 0.523941   0.361427   0.952\n",
            "Title (Cosine Similarity)       0.15 0.521739   0.358209   0.960\n",
            "Title (String Similarity)       0.30 0.499732   0.341392   0.932\n",
            "        Combined Original       0.20 0.488496   0.346444   0.828\n",
            "     Description (Cosine)       0.10 0.336711   0.275862   0.432\n",
            "    Description (Jaccard)       0.10 0.303797   0.357724   0.264\n",
            "Brand (String Similarity)       0.55 0.162003   0.307263   0.110\n",
            "Brand (Cosine Similarity)       0.10 0.161290   0.302198   0.110\n",
            "Evaluation completed in 4.72 seconds\n",
            "\n",
            "\n",
            "Generating comparison across all splits...\n",
            "================================================================================\n",
            "All evaluation results saved to: /content/drive/MyDrive/cs774/WDC/similarity_evaluation_20250427_220628\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}